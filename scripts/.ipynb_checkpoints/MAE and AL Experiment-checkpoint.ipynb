{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238c7015-e371-46d5-8fb1-2b8c00c1761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:45:05.282283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import sys\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from importlib import reload\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import src.data_processing.data_augmentation as daug\n",
    "import src.models.CNN as CNN\n",
    "reload(CNN)\n",
    "from src.models.active_learning_kcluster import kCenterGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d936f61-6424-40e0-99a4-947b11b7640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv(\"/Users/jameshill/PycharmProjects/bioacoustic-classifier/src/data/annotations/spectrogram_labels.csv\")\n",
    "df['filepath'] = \"/Users/jameshill/PycharmProjects/bioacoustic-classifier/data/processed/spectrogram_3s/\" + df['filename'] + \".png\"\n",
    "df['split_labels'] = df['label'].str.split('_and_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd493727-74b8-4fbb-a97a-a1834ef3ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrays\n",
    "filepaths = df['filepath'].values\n",
    "# Initialise and fit multi-label encoder\n",
    "mle = MultiLabelBinarizer()\n",
    "multi_labels = mle.fit_transform(df['split_labels'])\n",
    "labels = multi_labels\n",
    "cnn = CNN.define_cnn(mle.classes_)\n",
    "assert cnn.output_shape[-1] == len(mle.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f836acf-1990-433f-88db-6b7db3b65dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for MAE\n",
    "# DATA\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 64            \n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Spectrogram geometry\n",
    "SPEC_TIME = 512\n",
    "SPEC_FREQ = 64\n",
    "CHANNELS = 1\n",
    "INPUT_SHAPE = (SPEC_FREQ, SPEC_TIME, CHANNELS)\n",
    "\n",
    "NUM_CLASSES = len(mle.classes_) # for probe / classifier\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# PATCHING / MASKING\n",
    "# This is one dimension of the patch, so each patch is 16*16 = 256 pixels in size\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "# Spectrogram images are rectangular\n",
    "NUM_PATCHES = (SPEC_TIME // PATCH_SIZE) * (SPEC_FREQ // PATCH_SIZE) \n",
    "MASK_PROPORTION = 0.75\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "\n",
    "# ENCODER / DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 512\n",
    "ENC_NUM_HEADS = 8\n",
    "ENC_LAYERS = 8\n",
    "DEC_PROJECTION_DIM = 128\n",
    "DEC_NUM_HEADS = 8\n",
    "DEC_LAYERS = 2\n",
    "ENC_TRANSFORMER_UNITS = [ENC_PROJECTION_DIM * 2, ENC_PROJECTION_DIM]\n",
    "DEC_TRANSFORMER_UNITS = [DEC_PROJECTION_DIM * 2, DEC_PROJECTION_DIM]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd6596c-c2b4-487d-9466-841814755a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_with_aug(filepaths, labels, indices, flags, batch_size=32, seed=1929):\n",
    "    idx = np.asarray(indices, dtype=int)\n",
    "    labs = np.asarray(labels)[idx].astype('float32')\n",
    "    paths = np.asarray(filepaths)[idx]\n",
    "    flgs = np.asarray(flags).astype('bool')\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labs, flgs))\n",
    "    ds = ds.shuffle(len(idx), seed=seed, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(daug.decode_image_with_aug, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def make_ds_no_aug(filepaths, labels, indices, batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((filepaths[indices],\n",
    "                                             labels[indices].astype('float32')))\n",
    "    ds = ds.map(daug.decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0649b33-b4bf-4448-9661-70601bb36876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/ train split\n",
    "# Extract indices of data, test/ train split\n",
    "# This is so images themselves do not need duplicating\n",
    "# but instead augmentation will be applied when relevant index occurs\n",
    "idx = np.arange(len(filepaths))\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1929)\n",
    "rest_idx, test_idx = next(msss.split(idx, labels))\n",
    "train_idx, val_idx = next(msss.split(rest_idx, labels[rest_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72599026-197d-449e-94e5-5c7e0e8e95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the pretraining paradigm, the MAE will be trained on the training data and validated using validation data\n",
    "mae_train_ds = make_ds_no_aug(filepaths, labels, train_idx, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320e2a97-97eb-4499-a125-beefac3eb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Image has one channel so each patch would be\n",
    "        # of size (patch_size, patch_size, 1).\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * CHANNELS))\n",
    "\n",
    "    def call(self, images):\n",
    "        # Create patches from the input images\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "        # Reshape the patches to (batch, num_patches, patch_area) and return it.\n",
    "        patches = self.resize(patches)\n",
    "        return patches\n",
    "\n",
    "    def show_patched_image(self, images, patches):\n",
    "        # This is a utility function which accepts a batch of images and its\n",
    "        # corresponding patches and help visualize one image and its patches\n",
    "        # side by side.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.imshow(keras.utils.array_to_img(images[idx]))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        n_show = min(16, patches.shape[1])\n",
    "        side = int(np.ceil(np.sqrt(n_show)))\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(n_show):\n",
    "            ax = plt.subplot(side, side, i + 1)\n",
    "            patch_img = tf.reshape(patches[idx, i], (self.patch_size, self.patch_size, CHANNELS))\n",
    "            plt.imshow(tf.squeeze(patch_img), aspect=\"auto\", origin=\"lower\")\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"Showing {n_show} of {patches.shape[1]} patches\")\n",
    "        plt.show()\n",
    "        return idx\n",
    "\n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        rows = SPEC_FREQ // self.patch_size\n",
    "        cols = SPEC_TIME // self.patch_size\n",
    "        \n",
    "        p = self.patch_size\n",
    "        C = CHANNELS\n",
    "        patch = tf.reshape(patch, (rows, cols, p, p, C))\n",
    "        patch = tf.transpose(patch, [0, 2, 1, 3, 4])\n",
    "        reconstructed = tf.reshape(patch, (rows * p, cols * p, C))\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52839bc-4b38-4b4f-ac34-ed7eaf310ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index selected: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:29:50.792 Python[27347:11784397] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of images.\n",
    "image_batch, y_batch = next(iter(mae_train_ds))\n",
    "\n",
    "# Define the patch layer.\n",
    "patch_layer = Patches()\n",
    "\n",
    "# Get the patches from the batched images.\n",
    "patches = patch_layer(images=image_batch)\n",
    "\n",
    "# Now pass the images and the corresponding patches\n",
    "# to the `show_patched_image` method.\n",
    "random_index = patch_layer.show_patched_image(images=image_batch, patches=patches)\n",
    "\n",
    "# Chose the same chose image and try reconstructing the patches\n",
    "# into the original image.\n",
    "image = patch_layer.reconstruct_from_patch(patches[random_index])\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6df8990-12f3-488d-a89f-f7adff46d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        projection_dim=ENC_PROJECTION_DIM,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        downstream=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.downstream = downstream\n",
    "\n",
    "        # This is a trainable mask token initialized randomly from a normal\n",
    "        # distribution.\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([1, patch_size * patch_size * CHANNELS]), trainable=True\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_patches, self.patch_area) = input_shape\n",
    "\n",
    "        # Create the projection layer for the patches.\n",
    "        self.projection = layers.Dense(units=self.projection_dim)\n",
    "\n",
    "        # Create the positional embedding layer.\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "\n",
    "        # Number of patches that will be masked.\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "\n",
    "    def call(self, patches):\n",
    "        # Get the positional embeddings.\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [batch_size, 1, 1]\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # Embed the patches.\n",
    "        patch_embeddings = (\n",
    "            self.projection(patches) + pos_embeddings\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        if self.downstream:\n",
    "            return patch_embeddings\n",
    "        else:\n",
    "            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "            # The encoder input is the unmasked patch embeddings. Here we gather\n",
    "            # all the patches that should be unmasked.\n",
    "            unmasked_embeddings = tf.gather(\n",
    "                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "\n",
    "            # Get the unmasked and masked position embeddings. We will need them\n",
    "            # for the decoder.\n",
    "            unmasked_positions = tf.gather(\n",
    "                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "            masked_positions = tf.gather(\n",
    "                pos_embeddings, mask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, mask_numbers, projection_dim)\n",
    "\n",
    "            # Repeat the mask token number of mask times.\n",
    "            # Mask tokens replace the masks of the image.\n",
    "            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=0)\n",
    "            mask_tokens = tf.repeat(\n",
    "                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n",
    "            )\n",
    "\n",
    "            # Get the masked embeddings for the tokens.\n",
    "            masked_embeddings = self.projection(mask_tokens) + masked_positions\n",
    "            return (\n",
    "                unmasked_embeddings,  # Input to the encoder.\n",
    "                masked_embeddings,  # First part of input to the decoder.\n",
    "                unmasked_positions,  # Added to the encoder outputs.\n",
    "                mask_indices,  # The indices that were masked.\n",
    "                unmask_indices,  # The indices that were unmaksed.\n",
    "            )\n",
    "\n",
    "    def get_random_indices(self, batch_size):\n",
    "        # Create random indices from a uniform distribution and then split\n",
    "        # it into mask and unmask indices.\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, : self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask :]\n",
    "        return mask_indices, unmask_indices\n",
    "\n",
    "    def generate_masked_image(self, patches, unmask_indices):\n",
    "        # Choose a random patch and it corresponding unmask index.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "\n",
    "        # Build a numpy array of same shape as patch.\n",
    "        new_patch = np.zeros_like(patch)\n",
    "\n",
    "        # Iterate of the new_patch and plug the unmasked patches.\n",
    "        count = 0\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[unmask_index[i]] = patch[unmask_index[i]]\n",
    "        return new_patch, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ed221e-921d-4bf6-bff6-8aca676d6c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:46:06.674 Python[27753:11803149] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of images.\n",
    "image_batch, y_batch = next(iter(mae_train_ds))\n",
    "\n",
    "# Define the patch layer.\n",
    "patch_layer = Patches()\n",
    "\n",
    "# Get the patches from the batched images.\n",
    "patches = patch_layer(images=image_batch)\n",
    "# Create the patch encoder layer.\n",
    "patch_encoder = PatchEncoder()\n",
    "\n",
    "# Get the embeddings and positions.\n",
    "(\n",
    "    unmasked_embeddings,\n",
    "    masked_embeddings,\n",
    "    unmasked_positions,\n",
    "    mask_indices,\n",
    "    unmask_indices,\n",
    ") = patch_encoder(patches=patches)\n",
    "\n",
    "\n",
    "# Show a maksed patch image.\n",
    "new_patch, random_index = patch_encoder.generate_masked_image(patches, unmask_indices)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = patch_layer.reconstruct_from_patch(new_patch)\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img = image_batch[random_index]\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ddd4e-c568-4632-9759-833315337b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP encoder\n",
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7d78e-6ed0-4127-89d7-65eec4c092c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(num_heads=ENC_NUM_HEADS, num_layers=ENC_LAYERS):\n",
    "    inputs = layers.Input((None, ENC_PROJECTION_DIM))\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=ENC_PROJECTION_DIM // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=ENC_TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    outputs = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"mae_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ab6fa-36ca-4a84-b726-010f31362c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_decoder(\n",
    "    num_layers=DEC_LAYERS, num_heads=DEC_NUM_HEADS, image_size=IMAGE_SIZE\n",
    "):\n",
    "    inputs = layers.Input((None, ENC_PROJECTION_DIM))\n",
    "    x = layers.Dense(DEC_PROJECTION_DIM)(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=DEC_PROJECTION_DIM // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=DEC_TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "    outputs = layers.Dense(patch_area, activation=None)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"mae_decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f632be-ae75-445d-93e5-69516c6402ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_augmentation_model,\n",
    "        test_augmentation_model,\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_augmentation_model = train_augmentation_model\n",
    "        self.test_augmentation_model = test_augmentation_model\n",
    "        self.patch_layer = patch_layer\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def calculate_loss(self, images, test=False):\n",
    "        # Augment the input images.\n",
    "        if test:\n",
    "            augmented_images = self.test_augmentation_model(images)\n",
    "        else:\n",
    "            augmented_images = self.train_augmentation_model(images)\n",
    "\n",
    "        # Patch the augmented images.\n",
    "        patches = self.patch_layer(augmented_images)\n",
    "\n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(patches)\n",
    "\n",
    "        # Pass the unmaksed patche to the encoder.\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "\n",
    "        # Create the decoder inputs.\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "\n",
    "        # Decode the inputs.\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "        decoder_patches = self.patch_layer(decoder_outputs)\n",
    "\n",
    "        loss_patch = tf.gather(patches, mask_indices, axis=1, batch_dims=1)\n",
    "        loss_output = tf.gather(decoder_patches, mask_indices, axis=1, batch_dims=1)\n",
    "\n",
    "        # Compute the total loss.\n",
    "        total_loss = self.compute_loss(y=loss_patch, y_pred=loss_output)\n",
    "\n",
    "        return total_loss, loss_patch, loss_output\n",
    "\n",
    "    def train_step(self, images):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, loss_patch, loss_output = self.calculate_loss(images)\n",
    "\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.train_augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        tv_list = []\n",
    "        for grad, var in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "\n",
    "        # Report progress.\n",
    "        results = {}\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(loss_patch, loss_output)\n",
    "            results[metric.name] = metric.result()\n",
    "        return results\n",
    "\n",
    "    def test_step(self, images):\n",
    "        total_loss, loss_patch, loss_output = self.calculate_loss(images, test=True)\n",
    "\n",
    "        # Update the trackers.\n",
    "        results = {}\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(loss_patch, loss_output)\n",
    "            results[metric.name] = metric.result()\n",
    "        return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
